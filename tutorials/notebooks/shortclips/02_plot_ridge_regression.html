
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Understand ridge regression and cross-validation &#8212; Voxelwise modeling tutorials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tutorials/notebooks/shortclips/02_plot_ridge_regression';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Fit a ridge model with wordnet features" href="03_plot_wordnet_model.html" />
    <link rel="prev" title="Compute the explainable variance" href="01_plot_explainable_variance.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/flatmap.png" class="logo__image only-light" alt="Voxelwise modeling tutorials - Home"/>
    <script>document.write(`<img src="../../../_static/flatmap.png" class="logo__image only-dark" alt="Voxelwise modeling tutorials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    Voxelwise modeling tutorials
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="README.html">Shortclips tutorial</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="00_download_shortclips.html">Download the data set</a></li>
<li class="toctree-l2"><a class="reference internal" href="00_setup_colab.html">Setup Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="01_plot_explainable_variance.html">Compute the explainable variance</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Understand ridge regression and cross-validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_plot_wordnet_model.html">Fit a ridge model with wordnet features</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_plot_hemodynamic_response.html">Visualize the hemodynamic response</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_plot_motion_energy_model.html">Fit a ridge model with motion energy features</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_plot_banded_ridge_model.html">Fit a banded ridge model with both wordnet and motion energy features</a></li>
<li class="toctree-l2"><a class="reference internal" href="07_extract_motion_energy.html">Extract motion energy features from the stimuli</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../vim2/README.html">Vim-2 tutorial</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../vim2/00_download_vim2.html">Download the data set from CRCNS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../vim2/01_extract_motion_energy.html">Extract motion energy features from the stimuli</a></li>
<li class="toctree-l2"><a class="reference internal" href="../vim2/02_plot_ridge_model.html">Fit a ridge model with motion energy features</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../doc/voxelwise_modeling.html">Voxelwise modeling framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../doc/voxelwise_package.html">Helper Python package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../doc/references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Ftutorials/notebooks/shortclips/02_plot_ridge_regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/tutorials/notebooks/shortclips/02_plot_ridge_regression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Understand ridge regression and cross-validation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-least-squares-ols">Ordinary least squares (OLS)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression">Ridge regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-selection">Hyperparameter selection</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="understand-ridge-regression-and-cross-validation">
<h1>Understand ridge regression and cross-validation<a class="headerlink" href="#understand-ridge-regression-and-cross-validation" title="Link to this heading">#</a></h1>
<p>In future examples, we will model the fMRI responses using a regularized linear
regression known as <em>ridge regression</em>. This example explains why we use ridge
regression, and how to use cross-validation to select the appropriate
regularization hyper-parameter.</p>
<p>Linear regression is a method to model the relation between some input
variables <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{(n \times p)}\)</span> (the features) and an output
variable <span class="math notranslate nohighlight">\(y \in \mathbb{R}^{n}\)</span> (the target). Specifically, linear
regression uses a vector of coefficient <span class="math notranslate nohighlight">\(w \in \mathbb{R}^{p}\)</span>` to
predict the output</p>
<div class="math notranslate nohighlight">
\[
\begin{align}\hat{y} = Xw\end{align}
\]</div>
<p>The model is considered accurate if the predictions <span class="math notranslate nohighlight">\(\hat{y}\)</span> are close
to the true output values <span class="math notranslate nohighlight">\(y\)</span>. Therefore,  a good linear regression model
is given by the vector <span class="math notranslate nohighlight">\(w\)</span> that minimizes the sum of squared errors:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}w = \arg\min_w ||Xw - y||^2\end{align}
\]</div>
<p>This is the simplest model for linear regression, and it is known as <em>ordinary
least squares</em> (OLS).</p>
<section id="ordinary-least-squares-ols">
<h2>Ordinary least squares (OLS)<a class="headerlink" href="#ordinary-least-squares-ols" title="Link to this heading">#</a></h2>
<p>To illustrate OLS, let’s use a toy dataset with a single features <code class="docutils literal notranslate"><span class="pre">X[:,0]</span></code>.
On the plot below (left panel), each dot is a sample <code class="docutils literal notranslate"><span class="pre">(X[i,0],</span> <span class="pre">y[i])</span></code>, and
the linear regression model is the line <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">X[:,0]</span> <span class="pre">*</span> <span class="pre">w[0]</span></code>. On each
sample, the error between the prediction and the true value is shown by a
gray line. By summing the squared errors over all samples, we get the squared
loss. Plotting the squared loss for every value of <code class="docutils literal notranslate"><span class="pre">w</span></code> leads to a parabola
(right panel).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">voxelwise_tutorials.regression_toy</span> <span class="kn">import</span> <span class="n">create_regression_toy</span>
<span class="kn">from</span> <span class="nn">voxelwise_tutorials.regression_toy</span> <span class="kn">import</span> <span class="n">plot_1d</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_regression_toy</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plot_1d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/9c84f5d7cf11a719c3e6f9ea9dd5c28593347f7cf84d7aa1beb1126acf635fd4.png" src="../../../_images/9c84f5d7cf11a719c3e6f9ea9dd5c28593347f7cf84d7aa1beb1126acf635fd4.png" />
</div>
</div>
<p>By varying the linear coefficient <code class="docutils literal notranslate"><span class="pre">w</span></code>, we can change the prediction
accuracy of the model, and thus the squared loss.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_1d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="p">[</span><span class="mf">0.2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/7a1a0447e9a54932ea1cbd2ac0dae9e9566bd0b61759635df9a4c88e076cdfad.png" src="../../../_images/7a1a0447e9a54932ea1cbd2ac0dae9e9566bd0b61759635df9a4c88e076cdfad.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_1d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="p">[</span><span class="mf">0.7</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/f9c66c63b4d7f7ce9a7455648d2afe637113b6ce8d6e1e3fbf1e0127d566aba7.png" src="../../../_images/f9c66c63b4d7f7ce9a7455648d2afe637113b6ce8d6e1e3fbf1e0127d566aba7.png" />
</div>
</div>
<p>The linear coefficient leading to the minimum squared loss can be found
analytically with the formula:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}w = (X^\top X)^{-1}  X^\top y\end{align}
\]</div>
<p>This is the OLS solution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_ols</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plot_1d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="n">w_ols</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/9be4ce86faee4c15b59b039f9ae754ed6a7334e5938d8ba8a1fedc5e54baeab7.png" src="../../../_images/9be4ce86faee4c15b59b039f9ae754ed6a7334e5938d8ba8a1fedc5e54baeab7.png" />
</div>
</div>
<p>Linear regression can also be used on more than one feature. On the next toy
dataset, we will use two features <code class="docutils literal notranslate"><span class="pre">X[:,0]</span></code> and <code class="docutils literal notranslate"><span class="pre">X[:,1]</span></code>. The linear
regression model is a now plane. Here again, summing the squared errors over
all samples gives the squared loss.Plotting the squared loss for every value
of <code class="docutils literal notranslate"><span class="pre">w[0]</span></code> and <code class="docutils literal notranslate"><span class="pre">w[1]</span></code> leads to a 2D parabola (right panel).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">voxelwise_tutorials.regression_toy</span> <span class="kn">import</span> <span class="n">create_regression_toy</span>
<span class="kn">from</span> <span class="nn">voxelwise_tutorials.regression_toy</span> <span class="kn">import</span> <span class="n">plot_2d</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_regression_toy</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plot_2d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">show_noiseless</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/a44905b8be9b0e4b7d678bde97a0f7b1a20c76a25849bf2b165c2ca5e1a95dd2.png" src="../../../_images/a44905b8be9b0e4b7d678bde97a0f7b1a20c76a25849bf2b165c2ca5e1a95dd2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_2d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">show_noiseless</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/a43523402717467307915e1c0884f89a7dd0423dc8e6494833ae7fa6fe3283e4.png" src="../../../_images/a43523402717467307915e1c0884f89a7dd0423dc8e6494833ae7fa6fe3283e4.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_2d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="n">show_noiseless</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/1109f19f88f2dba3c6215aee5b3dd2a7189abc430076e1c02f9b128f24679348.png" src="../../../_images/1109f19f88f2dba3c6215aee5b3dd2a7189abc430076e1c02f9b128f24679348.png" />
</div>
</div>
<p>Here again, the OLS solution can be found analytically with the same formula.
Note that the OLS solution is not equal to the ground-truth coefficients used
to generate the toy dataset (black cross), because we added some noise to the
target values <code class="docutils literal notranslate"><span class="pre">y</span></code>. We want the solution we find to be as close as possible
to the ground-truth coefficients, because it will allow the regression to
generalize correctly to new data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_ols</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plot_2d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="n">w_ols</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/fbe61a45b12969635c790f5f1ae4fe3315617a1ff6198a2db60d75089c66d9fa.png" src="../../../_images/fbe61a45b12969635c790f5f1ae4fe3315617a1ff6198a2db60d75089c66d9fa.png" />
</div>
</div>
<p>The situation becomes more interesting when the features in <code class="docutils literal notranslate"><span class="pre">X</span></code> are
correlated. Here, we add a correlation between the first feature <code class="docutils literal notranslate"><span class="pre">X[:,</span> <span class="pre">0]</span></code>
and the second feature <code class="docutils literal notranslate"><span class="pre">X[:,</span> <span class="pre">1]</span></code>. With this correlation, the squared loss
function is no more isotropic, so the lines of equal loss are now ellipses
instead of circles. Thus, when starting from the OLS solution, moving <code class="docutils literal notranslate"><span class="pre">w</span></code>
toward the top left leads to a small change in the loss, whereas moving it
toward the top right leads to a large change in the loss. This anisotropy
makes the OLS solution less robust to noise in some particular directions
(deviating more from the ground-truth coefficients).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_regression_toy</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">correlation</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">w_ols</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plot_2d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="n">w_ols</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/585f6519c992920eb629200d5a6e97a2ff12a6a26a251342319611d2de57208c.png" src="../../../_images/585f6519c992920eb629200d5a6e97a2ff12a6a26a251342319611d2de57208c.png" />
</div>
</div>
<p>The different robustness to noise can be understood mathematically by the
fact that the OLS solution requires inverting the matrix <span class="math notranslate nohighlight">\((X^T X)\)</span>. The
matrix inversion amounts to inverting the eigenvalues <span class="math notranslate nohighlight">\(\lambda_k\)</span> of
the matrix. When the features are highly correlated, some eigenvalues
<span class="math notranslate nohighlight">\(\lambda_k\)</span> are close to zero, and a small change in the features can
have a large effect on the inverse. Thus, having small eigenvalues reduces
the stability of the inversion. If the correlation is even higher, the
smallest eigenvalues get closer to zero, and the OLS solution becomes even
less stable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_regression_toy</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">correlation</span><span class="o">=</span><span class="mf">0.999</span><span class="p">)</span>

<span class="n">w_ols</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plot_2d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="n">w_ols</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/e0c70d0b501854f202fbc8b76c8214a94b247021e96edba4758ecad970a62ced.png" src="../../../_images/e0c70d0b501854f202fbc8b76c8214a94b247021e96edba4758ecad970a62ced.png" />
</div>
</div>
<p>The instability can become even more pronounced with larger number of
features, or with smaller numbers of samples.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_regression_toy</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">correlation</span><span class="o">=</span><span class="mf">0.999</span><span class="p">)</span>

<span class="n">w_ols</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plot_2d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="n">w_ols</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/bdc8cc3ab62007aed8b86f28704de1f79b63f07bf9ed893a8925dfad07b7d7e4.png" src="../../../_images/bdc8cc3ab62007aed8b86f28704de1f79b63f07bf9ed893a8925dfad07b7d7e4.png" />
</div>
</div>
<p>When the number of features is larger than the number of samples, the linear
system becomes under-determined, which means that the OLS problem has an
infinite number of solutions, most of which do not generalize well to new
data.</p>
</section>
<section id="ridge-regression">
<h2>Ridge regression<a class="headerlink" href="#ridge-regression" title="Link to this heading">#</a></h2>
<p>To solve the instability and under-determinacy issues of OLS, OLS can be
extended to <em>ridge regression</em>. Ridge regression considers a different
optimization problem:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}w = \arg\min_w ||Xw - y||^2 + \alpha ||w||^2\end{align}
\]</div>
<p>This optimization problem contains two terms: (i) a <em>data-fitting term</em>
<span class="math notranslate nohighlight">\(||Xw - y||^2\)</span>, which ensures the regression correctly fits the
training data; and (ii) a regularization term <span class="math notranslate nohighlight">\(\alpha||w||^2\)</span>, which
forces the coefficients <span class="math notranslate nohighlight">\(w\)</span> to be close to zero. The regularization
term increases the stability of the solution, at the cost of a bias toward
zero.</p>
<p>In the regularization term, <code class="docutils literal notranslate"><span class="pre">alpha</span></code> is a positive hyperparameter that
controls the regularization strength. With a smaller <code class="docutils literal notranslate"><span class="pre">alpha</span></code>, the solution
will be closer to the OLS solution, and with a larger <code class="docutils literal notranslate"><span class="pre">alpha</span></code>, the solution
will be further from the OLS solution and closer to the origin.</p>
<p>To illustrate this effect, the following plot shows the ridge solution for a
particular value of <code class="docutils literal notranslate"><span class="pre">alpha</span></code>. The black circle corresponds to the line of
equal regularization, whereas the blue ellipses are the lines of equal
squared loss.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_regression_toy</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">correlation</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="mi">23</span>
<span class="n">w_ridge</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plot_2d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_ridge</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/e899cd7a54a018373b6890a055d9d0e0b3325523af36a15afc6ce6bfc2698095.png" src="../../../_images/e899cd7a54a018373b6890a055d9d0e0b3325523af36a15afc6ce6bfc2698095.png" />
</div>
</div>
<p>To understand why the regularization term makes the solution more robust to
noise, let’s consider the ridge solution. The ridge solution can be found
analytically with the formula:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}w = (X^\top X + \alpha I)^{-1}  X^\top y\end{align}
\]</div>
<p>where <code class="docutils literal notranslate"><span class="pre">I</span></code> is the identity matrix. In this formula, we can see that the
inverted matrix is now <span class="math notranslate nohighlight">\((X^\top X + \alpha I)\)</span>. Compared to OLS, the
additional term <span class="math notranslate nohighlight">\(\alpha I\)</span> adds a positive value <code class="docutils literal notranslate"><span class="pre">alpha</span></code> to all
eigenvalues <span class="math notranslate nohighlight">\(\lambda_k\)</span> of <span class="math notranslate nohighlight">\((X^\top X)\)</span> before the matrix
inversion. Inverting <span class="math notranslate nohighlight">\((\lambda_k + \alpha)\)</span> instead of
<span class="math notranslate nohighlight">\(\lambda_k\)</span> reduces the instability caused by small eigenvalues. This
explains why the ridge solution is more robust to noise than the OLS
solution.</p>
<p>In the following plots, we can see that even with a stronger correlation, the
ridge solution is still reasonably close to the noiseless ground truth, while
the OLS solution would be far off.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_regression_toy</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">correlation</span><span class="o">=</span><span class="mf">0.999</span><span class="p">)</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="mi">23</span>
<span class="n">w_ridge</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plot_2d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_ridge</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/7fce0030fff29b140c7651a8b8a6ad604b3ecd56443df20bb95b6b32665d0dea.png" src="../../../_images/7fce0030fff29b140c7651a8b8a6ad604b3ecd56443df20bb95b6b32665d0dea.png" />
</div>
</div>
<p>Changing the regularization hyperparameter <span class="math notranslate nohighlight">\(\alpha\)</span> leads to another
ridge solution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_regression_toy</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">correlation</span><span class="o">=</span><span class="mf">0.999</span><span class="p">)</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">w_ridge</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plot_2d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_ridge</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/ad1df2bd30fdd8be6487f2f3dec5324dcbc8fc10cdd406e79604a1d451659c51.png" src="../../../_images/ad1df2bd30fdd8be6487f2f3dec5324dcbc8fc10cdd406e79604a1d451659c51.png" />
</div>
</div>
<p>Side note: For every <span class="math notranslate nohighlight">\(\alpha\)</span>, at the corresponding ridge solution, the
line of equal regularization and the line of equal loss are tangent. If the
two lines were crossing, one could improve the ridge solution by moving along
one line. It would improve one term while keeping the other term constant.</p>
</section>
<section id="hyperparameter-selection">
<h2>Hyperparameter selection<a class="headerlink" href="#hyperparameter-selection" title="Link to this heading">#</a></h2>
<p>One issue with ridge regression is that the hyperparameter <span class="math notranslate nohighlight">\(\alpha\)</span> is
arbitrary. Different choices of hyperparameter lead to different models. To
compare these models, we cannot compare the ability to fit the training data,
because the best model would just be OLS (<span class="math notranslate nohighlight">\(alpha = 0\)</span>). Instead, we
want to compare the ability of each model to generalize to new data. To
estimate a model ability to generalize, we can compute its prediction
accuracy on a separate dataset that was not used during the model fitting
(i.e. not used to find the coefficients <span class="math notranslate nohighlight">\(w\)</span>).</p>
<p>To illustrate this idea, we can split the data into two subsets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">voxelwise_tutorials.regression_toy</span> <span class="kn">import</span> <span class="n">create_regression_toy</span>
<span class="kn">from</span> <span class="nn">voxelwise_tutorials.regression_toy</span> <span class="kn">import</span> <span class="n">plot_kfold2</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_regression_toy</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plot_kfold2</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">fit</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/992da344a9653a4904721fbf75dc7268278370322c25e5c389b12959b3b379a9.png" src="../../../_images/992da344a9653a4904721fbf75dc7268278370322c25e5c389b12959b3b379a9.png" />
</div>
</div>
<p>Then, we can fit a model on each subset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">plot_kfold2</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">fit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/2481106d3f3d199956c01660368b324829cb19150e287089d781dbb462f0e4bb.png" src="../../../_images/2481106d3f3d199956c01660368b324829cb19150e287089d781dbb462f0e4bb.png" />
</div>
</div>
<p>And compute the prediction accuracy of each model on the other subset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_kfold2</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">fit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">flip</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/b144cce408cedf21da90dfec6b7bb49004d9cefbff9741c876dd2875e4f0e932.png" src="../../../_images/b144cce408cedf21da90dfec6b7bb49004d9cefbff9741c876dd2875e4f0e932.png" />
</div>
</div>
<p>In this way, we can evaluate the ridge regression (fit with a specific
<span class="math notranslate nohighlight">\(\alpha\)</span>) on its ability to generalize to new data. If we do that for
different hyperparameter candidates <span class="math notranslate nohighlight">\(\alpha\)</span>, we can select the model
leading to the best out-of-set prediction accuracy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">voxelwise_tutorials.regression_toy</span> <span class="kn">import</span> <span class="n">plot_cv_path</span>

<span class="n">noise</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_regression_toy</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
<span class="n">plot_cv_path</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/eb0328192c8032a7b67e9ec6b119262dd886379c9991e555570310d6532763ff.png" src="../../../_images/eb0328192c8032a7b67e9ec6b119262dd886379c9991e555570310d6532763ff.png" />
</div>
</div>
<p>In the example above, the noise level is low, so the best hyperparameter
alpha is close to zero, and ridge regression is not much better than OLS.
However, if the dataset has more noise, a lower number of samples, or more
correlated features, the best hyperparameter can be higher. In this case,
ridge regression is better than OLS.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">noise</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_regression_toy</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
<span class="n">plot_cv_path</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/bf0196322bdd1b48a3ceaae9aaab3a6e3dad40d54f9d16c79844feb9e869ed6d.png" src="../../../_images/bf0196322bdd1b48a3ceaae9aaab3a6e3dad40d54f9d16c79844feb9e869ed6d.png" />
</div>
</div>
<p>When the noise level is too high, the best hyperparameter can be the largest
on the grid. It either means that the grid is too small, or that the
regression does not find a predictive link between the features and the
target. In this case, the model with the lowest generalization error always
predict zero (<span class="math notranslate nohighlight">\(w=0\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">noise</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_regression_toy</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
<span class="n">plot_cv_path</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/a8112a57d8bf0bbde9a9ca305ba0893bba1c41d9d6af20f265ab160bab98b432.png" src="../../../_images/a8112a57d8bf0bbde9a9ca305ba0893bba1c41d9d6af20f265ab160bab98b432.png" />
</div>
</div>
<p>To summarize, to select the best hyperparameter <span class="math notranslate nohighlight">\(\alpha\)</span>, the standard
method is to perform a grid search:</p>
<ul class="simple">
<li><p>Split the training set into two subsets: one subset used to fit the
models, and one subset to estimate the prediction accuracy (<em>validation
set</em>)</p></li>
<li><p>Define a number of hyperparameter candidates, for example [0.1, 1, 10,
100].</p></li>
<li><p>Fit a separate ridge model with each hyperparameter candidate
<span class="math notranslate nohighlight">\(\alpha\)</span>.</p></li>
<li><p>Compute the prediction accuracy on the validation set.</p></li>
<li><p>Select the hyperparameter candidate leading to the best validation
accuracy.</p></li>
</ul>
<p>To make the grid search less sensitive to the choice of how the training data
was split, the process can be repeated for multiple splits. Then, the
different prediction accuracies can be averaged over splits before the
hyperparameter selection. Thus, the process is called a <em>cross-validation</em>.</p>
<p>Learn more about hyperparameter selection and cross-validation on the
<a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html">scikit-learn documentation</a>.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./tutorials/notebooks/shortclips"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="01_plot_explainable_variance.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Compute the explainable variance</p>
      </div>
    </a>
    <a class="right-next"
       href="03_plot_wordnet_model.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Fit a ridge model with wordnet features</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-least-squares-ols">Ordinary least squares (OLS)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression">Ridge regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-selection">Hyperparameter selection</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gallant lab
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>